{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genius songs\n",
    "### Multivariate Data Analysis project\n",
    "Based on:<br>\n",
    "Derek Lim, Austin Benson. <br> _\"Expertise and Dynamics within Crowdsourced Musical Knowledge Curation: A Case Study of the Genius Platform.\"_ 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json_lines\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import enchant\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_raw = [json.loads(line) for line in open('song_info.json', 'r', encoding='utf-8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_info = pd.DataFrame(info_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_raw = []\n",
    "with open('lyrics.jl') as f:\n",
    "    for line in f:\n",
    "        lyrics_raw.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_info = pd.DataFrame(lyrics_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_name</th>\n",
       "      <th>title</th>\n",
       "      <th>primary_artist</th>\n",
       "      <th>release_date</th>\n",
       "      <th>pyongs</th>\n",
       "      <th>contributors</th>\n",
       "      <th>has_bio</th>\n",
       "      <th>views</th>\n",
       "      <th>tags</th>\n",
       "      <th>annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kendrick-lamar-swimming-pools-drank-lyrics</td>\n",
       "      <td>Swimming Pools (Drank)</td>\n",
       "      <td>Kendrick-lamar</td>\n",
       "      <td>July 31, 2012</td>\n",
       "      <td>894.0</td>\n",
       "      <td>403</td>\n",
       "      <td>True</td>\n",
       "      <td>5589280.0</td>\n",
       "      <td>[Trap, Conscious Hip-Hop, Memes, West Coast, R...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kendrick-lamar-money-trees-lyrics</td>\n",
       "      <td>Money Trees</td>\n",
       "      <td>Kendrick-lamar</td>\n",
       "      <td>October 22, 2012</td>\n",
       "      <td>880.0</td>\n",
       "      <td>394</td>\n",
       "      <td>True</td>\n",
       "      <td>4592003.0</td>\n",
       "      <td>[Conscious Hip-Hop, West Coast, Rap, Producer]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kendrick-lamar-xxx-lyrics</td>\n",
       "      <td>XXX.</td>\n",
       "      <td>Kendrick-lamar</td>\n",
       "      <td>April 14, 2017</td>\n",
       "      <td>188.0</td>\n",
       "      <td>389</td>\n",
       "      <td>True</td>\n",
       "      <td>4651514.0</td>\n",
       "      <td>[Conscious Hip-Hop, Boom Bap, Pop, West Coast,...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A-ap-rocky-fuckin-problems-lyrics</td>\n",
       "      <td>Fuckin’ Problems</td>\n",
       "      <td>A-ap-rocky</td>\n",
       "      <td>October 24, 2012</td>\n",
       "      <td>706.0</td>\n",
       "      <td>437</td>\n",
       "      <td>True</td>\n",
       "      <td>7378309.0</td>\n",
       "      <td>[Gangsta Rap, Dirty South, Atlanta, Posse Cut,...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kendrick-lamar-dna-lyrics</td>\n",
       "      <td>DNA.</td>\n",
       "      <td>Kendrick-lamar</td>\n",
       "      <td>April 14, 2017</td>\n",
       "      <td>555.0</td>\n",
       "      <td>570</td>\n",
       "      <td>True</td>\n",
       "      <td>5113687.0</td>\n",
       "      <td>[Politics, Producer, News, Conscious Hip-Hop, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     url_name                   title  \\\n",
       "0  Kendrick-lamar-swimming-pools-drank-lyrics  Swimming Pools (Drank)   \n",
       "1           Kendrick-lamar-money-trees-lyrics             Money Trees   \n",
       "2                   Kendrick-lamar-xxx-lyrics                    XXX.   \n",
       "3           A-ap-rocky-fuckin-problems-lyrics        Fuckin’ Problems   \n",
       "4                   Kendrick-lamar-dna-lyrics                    DNA.   \n",
       "\n",
       "   primary_artist      release_date  pyongs  contributors  has_bio      views  \\\n",
       "0  Kendrick-lamar     July 31, 2012   894.0           403     True  5589280.0   \n",
       "1  Kendrick-lamar  October 22, 2012   880.0           394     True  4592003.0   \n",
       "2  Kendrick-lamar    April 14, 2017   188.0           389     True  4651514.0   \n",
       "3      A-ap-rocky  October 24, 2012   706.0           437     True  7378309.0   \n",
       "4  Kendrick-lamar    April 14, 2017   555.0           570     True  5113687.0   \n",
       "\n",
       "                                                tags  annotations  \n",
       "0  [Trap, Conscious Hip-Hop, Memes, West Coast, R...          NaN  \n",
       "1     [Conscious Hip-Hop, West Coast, Rap, Producer]          NaN  \n",
       "2  [Conscious Hip-Hop, Boom Bap, Pop, West Coast,...          NaN  \n",
       "3  [Gangsta Rap, Dirty South, Atlanta, Posse Cut,...          NaN  \n",
       "4  [Politics, Producer, News, Conscious Hip-Hop, ...          NaN  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the lyrics to the song info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_info = lyrics_info.rename(columns = {'song':'url_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_process = pd.merge(lyrics_info, song_info, how=\"left\", on=[\"url_name\"])['lyrics'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This chunk of code is from the paper <br>\n",
    "(I modified it a bit to suit this task)_ <br>\n",
    "Removes part indicators and whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = [\"Intro\", \"Outro\", \"Chorus\", \"Hook\",\n",
    "            \"Pre-Hook\", \"Bridge\", \"Verse\", \"Refrain\",\n",
    "            \"Pre-Chorus\", \"Part\", \"Post-Chorus\", 'Interlude']\n",
    "re_parts = \"|\".join(map(lambda s: r\"\\b\" + s + r\"\\b\", parts))\n",
    "regex_parts = re.compile(r\"\\[(\"+re_parts+\").*\\]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37993/37993 [00:04<00:00, 9448.95it/s] \n"
     ]
    }
   ],
   "source": [
    "lyrics = []\n",
    "for song in tqdm(to_process):\n",
    "    song = re.sub(regex_parts, '', song)\n",
    "    song = re.sub('\\n', ' ', song)\n",
    "    song = re.sub('[\\(\\[].*?[\\)\\]]', '', song) # Remove adlibs and parts\n",
    "    song = re.sub('   ', ' ', song)\n",
    "    song = re.sub('  ', ' ', song)\n",
    "    song = re.sub('—', ' ', song)\n",
    "    song = song.replace('?', '')\n",
    "    song = song.replace('!', '')\n",
    "    song = song.replace(\"'\", '')\n",
    "    song = song.replace(\"'\", '')\n",
    "    song = song.replace(',' , '')\n",
    "    song = song.replace('’', '')\n",
    "    song = song.lower()\n",
    "    lyrics.append(song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stopwords and going along with the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lyrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9eba695542a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlyrics_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlyric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlyrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msplit_lyric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlyric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mresultwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit_lyric\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresultwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lyrics' is not defined"
     ]
    }
   ],
   "source": [
    "lyrics_clean = []\n",
    "for lyric in tqdm(lyrics):\n",
    "    split_lyric = lyric.split()\n",
    "    resultwords = [word for word in split_lyric if word not in stop_words and len(word) > 2]\n",
    "    result = ' '.join(resultwords)\n",
    "    lyrics_clean.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the lemmatizer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the vectorizer: maximal features is the number of words it will count, try with _20_ and continue with no limit (maybe overnight), max and min df will specify what the range of proportion the words that appear have to be in ngram_range could be 2 (?) <- to consider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge lyrics to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_info['lyrics'] = lyrics_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(lyrics_info, song_info, how=\"left\", on=[\"url_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove non-English lyrics (or songs with less than 70% English words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37993it [01:13, 520.21it/s]\n"
     ]
    }
   ],
   "source": [
    "enwords = []\n",
    "nonenwords = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    split_lyric = row['lyrics'].split()\n",
    "    n_enword = 0\n",
    "    n_nonenword = 0\n",
    "    for word in split_lyric:\n",
    "        if d.check(str(word)) == True:\n",
    "            n_enword += 1\n",
    "        else: \n",
    "            n_nonenword += 1\n",
    "    enwords.append(n_enword)\n",
    "    nonenwords.append(n_nonenword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['enwords'] = [enwords[i]/(enwords[i]+nonenwords[i]) \n",
    "                          if enwords[i] != 0 and nonenwords[i] !=0\n",
    "                          else 0\n",
    "                          for i in range(len(enwords))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Load and save_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('unfiltered_dataframe.csv', index=False)\n",
    "df = pd.read_csv('unfiltered_dataframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportion of English words should be more than 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['enwords'] > 0.7]\n",
    "df = df[df['views'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the vectorizer, specify the number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lemmatizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, df):\n",
    "        return [self.wnl.lemmatize(word) for word in word_tokenize(df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=1000, max_df=0.8, min_df=0.05,\n",
    "                             tokenizer=lemmatizer(), lowercase=True, stop_words='english',\n",
    "                             token_pattern = r'w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/www/rajkjupyter/.local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_df=0.8, max_features=1000, min_df=0.05,\n",
       "                stop_words='english', token_pattern='w+',\n",
       "                tokenizer=<__main__.lemmatizer object at 0x7f87966c6910>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(df['lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0        pour head shot sit stand pass wake faded faded...\\n1        niggas tryna get bish hit house lick: tell wit...\\n2        america god bless good america please take han...\\n3        love bad bitches thats fuckin problem yeah lik...\\n4        got got got got loyalty got royalty inside dna...\\n                               ...                        \\n37952    stone wall dog gaze duct taped ceiling stucco ...\\n37953    headless skid like rita lifted ocean drift lik...\\n37973    dont want club rather sit couch watch movie gi...\\n37974    say something wrong hear thinking talk way lon...\\n37992    ayy wild things youre doin night trips whereve...\\nName: lyrics, Length: 34445, dtype: object'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lyrics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector = vectorizer.transform(df['lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>''</th>\n",
       "      <th>.</th>\n",
       "      <th>...</th>\n",
       "      <th>:</th>\n",
       "      <th>;</th>\n",
       "      <th>``</th>\n",
       "      <th>act</th>\n",
       "      <th>aint</th>\n",
       "      <th>air</th>\n",
       "      <th>arm</th>\n",
       "      <th>...</th>\n",
       "      <th>wrong</th>\n",
       "      <th>yall</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>youll</th>\n",
       "      <th>young</th>\n",
       "      <th>youre</th>\n",
       "      <th>youve</th>\n",
       "      <th>‘</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 486 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ''   .  ...  :  ;  ``  act  aint  air  arm  ...  wrong  yall  yeah  year  \\\n",
       "0   0   0    0  0  0   0    0     1    0    0  ...      0     0     1     0   \n",
       "1   1  12    0  5  0   1    0     3    0    0  ...      0     0     0     0   \n",
       "2   4   2    0  2  1   3    0     2    0    0  ...      0     0     0     0   \n",
       "3   0   1    0  0  3   0    1     3    0    0  ...      0     0    13     0   \n",
       "4   0   0    0  0  0   0    0     4    0    0  ...      0     0     5     1   \n",
       "\n",
       "   yes  youll  young  youre  youve  ‘  \n",
       "0    0      0      0      2      0  0  \n",
       "1    0      0      0      0      0  0  \n",
       "2    0      0      0      0      0  0  \n",
       "3    1      0      0      0      0  1  \n",
       "4    0      0      1      0      0  0  \n",
       "\n",
       "[5 rows x 486 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = count_vector.toarray()\n",
    "bow = pd.DataFrame(bow, columns=vectorizer.get_feature_names())\n",
    "bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the BOW df\n",
    "#bow.to_csv('bag_of_words.csv', index=False)\n",
    "bow = pd.read_csv('bag_of_words.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the weirdo words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_filtered = bow.iloc[:, 6:996]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnecessary = ['aint', 'ayy', 'baby', 'yall', 'yeah', 'yes', 'youre', 'youve', 'youll', 'big', 'bitch', 'bos', \n",
    "               'caught', 'chick', 'comin', 'damn', 'doin', 'dont', 'dude', 'feelin', 'fuck', 'fucked', \n",
    "               'fuckin', 'fucking', 'gettin', 'getting', 'goin', 'going', 'gon', 'gone', 'hey', 'hoe', \n",
    "               'huh', 'ima', 'imma', 'ive', 'let', 'lookin', 'man', 'motherfucker', 'na', 'nah', 'nigga', \n",
    "               'nothin', 'okay', 'ooh', 'pas', 'playin', 'really', 'sayin', 'shes', 'smokin', 'somethin', \n",
    "               'son', 'ta', 'talkin', 'thats', 'theyre', 'thinkin', 'til', 'till', 'took', 'tryin', 'tryna', \n",
    "               'wan', 'want', 'wasnt', 'whats', 'wont', 'wouldnt', '‘']\n",
    "\n",
    "for column_name in unnecessary:\n",
    "    del bow_filtered[column_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are pre-trained language models, e.g. TextBlob for this purpose, however, since lyrics are very specific linguistically, I will train my own model on the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/www/rajkjupyter/.local/lib/python3.7/site-packages/ipykernel_launcher.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/var/www/rajkjupyter/.local/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "bow_filtered['no_of_views'] = song_info['views']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>air</th>\n",
       "      <th>arm</th>\n",
       "      <th>ask</th>\n",
       "      <th>away</th>\n",
       "      <th>bad</th>\n",
       "      <th>bag</th>\n",
       "      <th>ball</th>\n",
       "      <th>band</th>\n",
       "      <th>bank</th>\n",
       "      <th>...</th>\n",
       "      <th>word</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>worth</th>\n",
       "      <th>wrist</th>\n",
       "      <th>write</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>young</th>\n",
       "      <th>no_of_views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5589280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4592003.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4651514.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7378309.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5113687.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 410 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   act  air  arm  ask  away  bad  bag  ball  band  bank  ...  word  work  \\\n",
       "0    0    0    0    0     0    1    0     0     0     0  ...     0     0   \n",
       "1    0    0    0    0     0    0    0     0     1     0  ...     0     1   \n",
       "2    0    0    0    1     0    0    0     0     0     1  ...     0     0   \n",
       "3    1    0    0    0     0   12    0     1     0     0  ...     1     0   \n",
       "4    0    0    0    0     0    0    0     0     0     0  ...     0     2   \n",
       "\n",
       "   world  worth  wrist  write  wrong  year  young  no_of_views  \n",
       "0      0      0      0      0      0     0      0    5589280.0  \n",
       "1      0      0      0      0      0     0      0    4592003.0  \n",
       "2      0      0      0      0      0     0      0    4651514.0  \n",
       "3      1      0      0      0      0     0      0    7378309.0  \n",
       "4      1      0      0      0      0     1      1    5113687.0  \n",
       "\n",
       "[5 rows x 410 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_filt['genre_of_song'] = song_info['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_tags = ['Country', 'R&B', 'Rap', 'Rock', 'Pop'] #Pop is the default main tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:09<00:00,  1.82s/it]\n"
     ]
    }
   ],
   "source": [
    "tag_list = []\n",
    "for tag in tqdm(main_tags):\n",
    "        for index, row in bow_filt.iterrows():\n",
    "            if tag in row['genre_of_song']:\n",
    "                tag_list.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34445it [00:02, 14346.31it/s]\n"
     ]
    }
   ],
   "source": [
    "main_tags_list = []\n",
    "for index, row in bow_filt.iterrows():\n",
    "    main = None\n",
    "    for tag in main_tags:\n",
    "        if tag in row['genre_of_song']:\n",
    "            main = tag\n",
    "    main_tags_list.append(main)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_filt['main_tag'] = main_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_filt.drop('genre_of_song', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_filt.to_csv('filtered_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
